# -*- coding: utf-8 -*-
"""FineTuningQwenForDoctorChat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O9rEkOOONFQDyEr-gl8HLBFHQh-6bqao
"""

# Install all required packages
!pip install -q transformers accelerate datasets peft trl bitsandbytes huggingface_hub evaluate

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Hugging Face login
from huggingface_hub import login
login()

import json
from datasets import Dataset

# Load iCliniq dataset from Drive
with open("/content/drive/MyDrive/iCliniq.json", "r") as f:
    raw_data = json.load(f)

# Format for instruction tuning
formatted_data = []
for item in raw_data:
    question = item.get("input", "").strip()
    answer = item.get("answer_icliniq", "").strip()
    if question and answer:
        formatted_data.append({
            "instruction": question,
            "input": "",
            "output": answer
        })

print("Valid samples:", len(formatted_data))

# Create DatasetDict
dataset = Dataset.from_list(formatted_data)
dataset = dataset.train_test_split(test_size=0.05, seed=42)

from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "Qwen/Qwen1.5-1.8B"

tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype="auto",
    trust_remote_code=True
)

def tokenize(batch):
    full_prompt = [
        f"<|user|>\n{instruction}\n<|assistant|>\n{output}"
        for instruction, output in zip(batch["instruction"], batch["output"])
    ]
    return tokenizer(full_prompt, truncation=True, padding="max_length", max_length=512)

tokenized_dataset = dataset.map(tokenize, batched=True)
print(tokenized_dataset)

from peft import get_peft_model, LoraConfig, TaskType

peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

from transformers import TrainingArguments, TrainerCallback, TrainerState, TrainerControl

class EpochLogger(TrainerCallback):
    def on_epoch_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):
        # Safely find latest log that contains 'loss'
        logs = [log for log in reversed(state.log_history) if 'loss' in log]
        if logs:
            loss = logs[0]['loss']
            print(f"ðŸ“Š Epoch {int(state.epoch)} | Loss: {loss:.4f}")
        else:
            print(f"ðŸ“Š Epoch {int(state.epoch)} | Loss not available.")


training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/qwen-finetuned-doctor",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    logging_strategy="steps",
    logging_steps=50,
    save_strategy="epoch",
    report_to=[],
    fp16=True,
    push_to_hub=False
)

from transformers import DataCollatorForLanguageModeling
from trl import SFTTrainer

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    data_collator=data_collator,
    compute_metrics=None,
    callbacks=[EpochLogger()]
)

train_output = trainer.train()
print(train_output)

# Save model and tokenizer to your preferred path
save_path = "/content/drive/MyDrive/qwen-finetuned-doctor"

model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print(f" Model and tokenizer saved to {save_path}")

import torch

eval_results = trainer.evaluate()
eval_loss = eval_results["eval_loss"]
perplexity = torch.exp(torch.tensor(eval_loss))

print(f" Eval Loss: {eval_loss:.4f}")
print(f" Perplexity: {perplexity:.2f}")

!pip install -q bert_score

def ask_doctor(question, max_new_tokens=256):
    prompt = f"### Instruction:\n{question}\n### Response:"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        top_p=0.9,
        temperature=0.7,
        repetition_penalty=1.2,
        eos_token_id=tokenizer.eos_token_id
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(prompt):].strip()

from bert_score import score
from tqdm import tqdm

# Sample size (you can increase this to 50+ for better estimation)
sample_size = 20
references = []
candidates = []

for i in tqdm(range(sample_size)):
    question = tokenized_dataset["test"][i]['instruction']
    true_answer = tokenized_dataset["test"][i]['output'].strip()
    generated_answer = ask_doctor(question).strip()

    # Avoid blank generations (prevent warnings)
    if not generated_answer:
        generated_answer = "[EMPTY]"

    references.append(true_answer)
    candidates.append(generated_answer)

# Compute BERTScore
P, R, F1 = score(candidates, references, lang="en", verbose=True)

# Print results
print(f"\n Average BERTScore F1: {F1.mean():.4f}")

from bert_score import score
from tqdm import tqdm

# Number of samples to evaluate
sample_size = 20
references = []
candidates = []
questions = []

for i in tqdm(range(sample_size)):
    question = tokenized_dataset["test"][i]['instruction']
    true_answer = tokenized_dataset["test"][i]['output'].strip()
    generated_answer = ask_doctor(question).strip()

    if not generated_answer:
        generated_answer = "[EMPTY]"

    references.append(true_answer)
    candidates.append(generated_answer)
    questions.append(question)

# Compute BERTScore
P, R, F1 = score(candidates, references, lang="en", verbose=True)

# Display each generation with its F1 score
for i in range(sample_size):
    print(f"\n Question: {questions[i]}")
    print(f" Model Answer: {candidates[i]}")
    print(f" Ground Truth: {references[i]}")
    print(f" BERTScore F1: {F1[i].item():.4f}")

# Show average score at the end
print(f"\n Average BERTScore F1: {F1.mean():.4f}")